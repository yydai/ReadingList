* Chapter 1 什么是好的推荐系统
    需要考虑三个方面的内容：用户、内容提供方和推荐网站

    预测准确度是推荐系统领域的重要指标（没有之一），但是推荐准确不代表是好的推荐。

    所以好的推荐应该是不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现他们可能感兴趣的东西。

    为了全面评测这里提出了很多的指标包括：准确度、覆盖度、新颖度、惊喜度、信任度、透明度。

    推荐系统有三个实验方法：
    1. 离线实验
    2. 用户调查 代价比较的高
    3. 在线实验 进行 AB 测试

    评价的指标：
** 用户的满意度
        调查问卷形式进行
** 预测的准确度
    1. 评分预测的方法有：均方误差（RMSE），平均绝对误差(MAE)。
        比如 records 存放用户的评分列表 records[i] = [u, i, rui, pui]
        rui 是用户对物品的实际评分，pui 是算法预测出来的用户 u 对物品 i 的评分。
        RMSE:
        #+BEGIN_SRC python
        def rmse(records):
            return math.sqrt(sum([(rui - pui) ** 2 for u, i, rui, pui in records])) / float(len(records))
        def MAE(records):
            return sum([(rui-pui)**2 for u, i, rui, pui in records]) / float(len(records))
        #+END_SRC
    2. TopN 推荐
        预测的准确率会通过准确率（precision）和召回率（recall）度量
        #+BEGIN_SRC python
        def PrecisionRecall(test, N):
            hit = 0
            n_recall = 0
            n_precision = 0
            for user, items in test.items():
                rank = Recommend(user, N)
                hit += len(rank & items)
                n_recall += len(items)
                n_precision += N
            return [hit/(1.0 * n_recall), hit / (1.0 * n_precision)]
        #+END_SRC
** 覆盖率（coverage）
    覆盖率描述了推荐系统发掘长尾的能力，统计不同物品被推荐的次数分布，
    如果比较的平那么覆盖率比较的高，否则就低。可以通过信息熵来描述它。

    H = - SUM p(i)*logp(i)

    另外一个是基尼系数：
    G = 1 / (n-1) sum(2j - n - 1) p(i_j)

    #+BEGIN_SRC python
    def GiniIndex(p):
        j = 1
        n = len(p)
        G = 0
        for item, weight in sorted(p.items(), key=itemgetter(1)):
            G += sum(2 * j - n - 1) * weight
            j += 1
        return G / float(n-1)
    #+END_SRC

    社会学中有一个著名的马太效应：所谓强者更强，弱者更弱。

    推荐算法是否具有马太效用也是一个比较关注的问题。我们的期望是能够消除它，但是实际上很难。

** 多样性
     多样性描述了推荐列表中物品两两之间的不相似性。因此，多样性和相似
 性是对应的。假设 s(i, j) [0,1] 定义了物品 i 和 j 之间的相似度，那么用户 u
 的推荐列表 R(u)的多样性定义如下:

** 新颖性
    新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。

    评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物
品越 可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，
那么推荐结果就可能有比较高的新颖性。

** 惊喜度
    令用户惊喜的推荐结果是和用户历史上喜欢的物品不相似，但用户却觉得满意的推荐。

* Chapter 2 利用用户行为数据
    我们为了深入的了解用户，最理想的情况是在注册的时候主动告诉我们用户
    喜欢什么，但是缺点也很明显：
    - 自然语言描述很难理解
    - 用户兴趣不断的更新
    - 用户有时候并不知道喜欢什么

    购买 A 商品的用户都购买 B 商品。

    用户行为在个性化推荐中，显性反馈行为，隐形反馈行为

    用户行为的统一表示：
    user_id item_id
    behavior_type: 购买还是浏览
    context：产生行为的上下文，包括时间和地点
    behavior_weight：行为的权重，例如观看视频的时间长度，听歌曲的次数
    behavior_content： 行为的内容，如果是评论信息，就是评论的文本，打标签的行为就是标签

** 用户活跃度和物品流行度分布
    互联网中的很多数据都满足一种叫做 Power Law 分布，也就是叫做长尾分布。

** 用户活跃度和物品流行度的关系
    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_131628_29803Aew.png]]

    仅仅基于用户行为数据设计的推荐算法一般称为协同过滤算法。 关于它有
    很多的算法，如基于邻域的的方法，隐语义模型，用的最多的还是基于邻域的方法：
    - 基于用户的协同过滤算法
    - 基于五批的协同过滤算法

    评测指标：
    召回率 准确率
    召回率：描述了有多少比例的用户-物品评分记录包含在最终的推荐列表中
    准确率：描述推荐列表中有多少比例是
    除了这个还有覆盖率，覆盖率越高说明算法越能推荐长尾中的物品给用户。

    #+BEGIN_SRC python
    def Coverage(train, test, N):
        recommend_items = set()
        all_items = set()
        for user in train.keys():
            for item in train[user].keys():
                all_items.add(item)
            rank = GetRecommendation(user, N)
            for item, pui in rank:
                recommend_items.add(item)
        return len(recommend_items) / (len(all_items) * 1.0)
    #+END_SRC

** 基于用户的协同过滤算法
    1. 找到和目标用户兴趣相似的用户集合
    2. 找到这个集合中的用户喜欢的，并且目标用户没有听说过的物品推荐给目标用户

    步骤一的关键是计算两个用户兴趣的相似度，这里主要是利用行为的相似度
    计算兴趣的相似度。这里我们评价的公式有 Jaccard 公式：
    N(u) 表示用户 u 有过正反馈的物品集合，N(v) 为用户 v 有过正反馈的物品集合。
    w_{uv} = N(u) 和 N(v) 的交集 / 它们的并集

    或者通过余弦公式：
    #+ATTR_HTML: :width 40%
    [[file:./imgs/20180415_141254_29803_xF.png]]

    下面是个例子：
    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_141316_29803M8L.png]]

    实现的伪代码：
    #+BEGIN_SRC python
    def Similarity(train):
        w = dict()
        for u in train.keys():
            for v in train.keys():
                if u == v:
                    continue
                w[u][v] = len(train[u] & train[v])
                w[u][v] /= math.sqrt(len(train[u] * len(train[v])) * 1.0)
        return w
    #+END_SRC

    上面的计算方法有个很大的问题就是时间复杂度太高，为 O(n*n), 当用户数很大时是比较耗时的。

    如果换一种思路，我们首先计算 |N(u) and N(v)| != 0 的用户对(u, v), 然后再计算分母


    首先建立，物品-用户倒排表，然后建立用户相识度 w 矩阵：
    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_170901_29803mQY.png]]

    UserCF 算法：
    p(u, i) = sum(w_uv * r_vi) where v belong to s(u,k)and N(i)

    s(u, k) 为用户和 u 兴趣最接近的 k 个用户，N(i) 对物品 i 有过行为的用户集合。w_ui 为 u 和 v 的兴趣相似度。r_vi 代表用户 v 对物品 i 的兴趣。

    代码：
    #+BEGIN_SRC python
    def Recommend(user, train, w):
        rank = dict()
        iteracted_items = train[user]
        for v, wuv in sorted(w[u].items, key=itemgetter(1), reverse=True)[0:K]:
            for i, rvi in train[v].items:
                if i in interacted_items:
                    continue
                rank[i] += wuv * rvi
        return rank
    #+END_SRC


    User-IIF 算法：
    这里会考虑一下流行度的影响，也就是加一个罚项，减小一下非常热门的影响。

    #+BEGIN_SRC python

    #+END_SRC

    基于用户的协同过滤算法在实际过程中用的不是太多，原因是用户数目越来
    越大，用户的兴趣矩阵计算会非常的困难，运行的时间复杂度和空间复杂度
    为平方关系。另外，基于用户的协同过滤很难对推荐结果作出解释。
    所以亚马逊提出了一个基于物品的协同过滤算法。

** 基于物品的协同过滤算法
    它不会通过物品的内容去计算相识度，它主要通过分析用户的购买记录。也
    就是说，如果用户喜欢 A 物品，也喜欢 B 物品，那么 A 和 B 就具有很高的相识
    度。

    1. 计算物品的相识度
    2. 根据物品的相识度和用户的历史行为给用户生成推荐列表

    我们可以定义物品的相识度：
    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_173433_29803zae.png]]

    上述公式可以理解为喜欢物品 i 的用户中有多少比例的用户也喜欢物品 j。

    如果物品 j 很热门，很多人都喜欢，那么 Wij 就会很大，接近 1。因此，该公
    式会造成任何物品都会和热门的物品有很大的相似度，这 对于致力于挖掘
    长尾信息的推荐系统来说显然不是一个好的特性。为了避免推荐出热门的物
    品， 可以用下面的公式:

    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_173537_29803Alk.png]]

    这个公式惩罚了物品 j 的权重，因此减轻了热门物品会和很多物品相似的可能性。

    这里也要考虑用户活跃度对推荐结果的影响。
    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_182954_29803Nvq.png]]

** 归一化操作
    研究发现如果对 Item 的相识度矩阵按照最大值进行归一化操作，可以提高推荐的准确率。

    Wij = Wij / max(Wij)

** 隐语义模型
    LFM（latent factor model）, 用于找到文本的隐含语义。相关的名词有 LDA, Topic Model。
    这里我们讨论一下在 Top-N 推荐中的应用。

    // 除了上面的 UserCF 和 ItemCF，我们还要看看

    LFM 诞生到今天出现了很多的技术，例如：LDA, 隐含类别模型，隐含主题模型，矩阵分解等。

** 基于图的推荐算法
    skip

* 推荐系统冷启动问题
    分为三类：用户冷启动，物品冷启动，系统冷启动

    解决方案可以这样，先给用户推荐热门排行榜，然后收集到用户行为后，再进行个性化推荐。

    还有是当新用户访 问推荐系统时，它会给出一条提示语，表示用户需要给多部电影评分才能获取推荐结果。

    对于新加入的物品，我们可以通过物品的内容来计算它的相似度。不同的物品有不同的内容信息，例如电影会有标题导演，风格，剧情，国家等。

    一般我们通过向量空间模型来表示，该模型会将物品表示成一个关键词向量。 过程一般如下：

    #+ATTR_HTML: :width 80%
    [[file:./imgs/20180415_193934_29803a5w.png]]

    d_i = {(e1, w1), (e2, w2),...}

    其中，e_i 就是关键词，w_i 是关键词对应的权重。

    如果物品是文本，我们可以通过著名的 tf-idf 公司计算权重。

    w_i = tf(e_i) / log(DF(e_i))

    tf 就是词出现的次数，

    tf-idf = 词频 (tf) * idf

    IDF 为所有文档的数目除以包含该词语的文档数目的对数值。

    计算出相似度以后，我们就需要通过余弦公式来计算两个文档间的相似度了。

    但是这里有个问题，就是时间复杂度会非常的高

    在实际应用中，我们首先通过建立关键词-物品的倒排表来加速这个过程： 如何做的？好好理解一下这里？为什么可以降低时间复杂度？

    对于长文本来说，使用向量空间模型可以计算得到很高的精确度，但是如果文本很短的话，关键词很少，向量空间模型就很难就算出准确的相似度了。

    我们下面讨论的模型是话题模型(topic model) 使用 LDA（Latent Dirichilet Allocation）算法，

    LDA 有 3 中元素，文档、话题和词语。每一篇文档都会表现为词的集合，这个称为词袋模型。

    这部分没看很懂，多查些资料。。。。。

* 利用用户标签数据
    当一 个用户对一个物品打上一个标签，这个标签一方面描述了用户的兴趣，
    另一方面则表示了物品的 语义，从而将用户和物品联系了起来。

    这里我们讨论的主要是用户给物品打标签，也就是 UGC(User Generated Content


    矩阵分解算法。


1. 项目中遇到了问题，时间复杂度高， 而且比较的稀疏？使用了倒排索引来解决了这个问题。

2. LDA 的使用

3. 用户的冷启动问题？ 用户的个人信息，性别，兴趣，个人的健康档案信息

4. TOP K 推荐问题？如何解决的？
